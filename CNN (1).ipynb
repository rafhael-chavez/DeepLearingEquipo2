{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiI8YMS5njSS",
        "outputId": "2e38ee77-08a0-42ec-9ed2-a3d5f01504bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.2)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import LambdaCallback"
      ],
      "metadata": {
        "id": "h_cNMBC7n5Py"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción\n",
        "En este dataset, se analizan una cantidad de imágenes, esto se consiguió en la propia librería de TenserFlow, ya que al tratar de usar las de kaggle son demasiados ejemplos por lo que el archivo se vuelve muy pesado. El dataset trata de clasificar las imágenes, en los siguientes campos, aviones, autos, pájaros, gatos, venados, perros, rana, caballos, barco y camiones. Eso en cuanto a las neuronas convulcionales, después lo que se requiere es usar el Data Aumentation, para crear mucho más imágenes, variando multiples factores de esta como la posición, desplazar y rotar. El uso de Data Augmentation no solo aumenta el tamaño del conjunto de datos sino que también introduce variabilidad, lo que permite que el modelo sea más robusto y generalice mejor en datos no vistos.\n",
        "# Objetivos del Notebook\n",
        "- Crear un modelo de CNN\n",
        "- Ver el impacto del Data Augmentation en el modelo\n",
        "- Saber interpretar los resultados en ambos casos del modelo\n"
      ],
      "metadata": {
        "id": "L0pRJxCD2k3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "# Normalizar las imágenes a valores entre 0 y 1 para que se pueda hacer el entrenamiento y pruebas en el modelo, luego de está limpieza\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.astype('float32') / 255"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3YjVU1Xo8Ur",
        "outputId": "155c2192-13f9-41c6-8515-d97a95f68d6a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# En el dataset se puede encontro 50000 imágenes que están distribuidas en las 10 categorías anteriores de 32x32 y a color\n",
        "print(train_images.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uB54l-GzafL",
        "outputId": "866b8504-f94a-4413-c597-014afb1412ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un modelo\n",
        "model = models.Sequential()\n",
        "\n",
        "# Primera capa convolucional: 32 filtros de 3x3, activación ReLU, con imágenes de entrada de 32x32x3\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# Segunda capa convolucional con 64 filtros\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# Tercera capa convolucional con 64 filtros\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "# Aplanar la salida para las capas densas\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# Agregar una capa densa con 64 neuronas\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "# Capa de salida con 10 neuronas (correspondiente a las 10 clases de CIFAR-10), usando softmax\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTAe79l_pSa3",
        "outputId": "b4f02bf1-dfe0-4662-f952-31b085e4ffbe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Crear un callback para mostrar precisión y error por época\n",
        "metrics_callback = LambdaCallback(on_epoch_end=lambda epoch, logs:\n",
        "                                  print(f'Epoch {epoch + 1}: Precisión = {logs[\"accuracy\"] * 100:.2f}%, Error = {(1 - logs[\"accuracy\"]) * 100:.2f}%'))\n",
        "\n",
        "# Entrenar el modelo sin Data Augmentation\n",
        "print(\"Entrenamiento sin Data Augmentation\")\n",
        "history_no_augmentation = model.fit(train_images, train_labels,\n",
        "                                    epochs=10,\n",
        "                                    batch_size=64,\n",
        "                                    callbacks=[metrics_callback])\n",
        "\n",
        "# Evaluar el modelo sin Data Augmentation\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f'\\nTest accuracy sin Data Augmentation: {test_acc}')\n",
        "\n",
        "# Guardar los pesos iniciales del modelo antes de reiniciarlo\n",
        "initial_weights = model.get_weights()\n",
        "\n",
        "# Reiniciar los pesos del modelo\n",
        "model.set_weights(initial_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SeLNAZKpkot",
        "outputId": "4f96a05f-6ec1-4ac9-86ef-8ead224b8fca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenamiento sin Data Augmentation\n",
            "Epoch 1/10\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.3222 - loss: 1.8468Epoch 1: Precisión = 41.20%, Error = 58.80%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 86ms/step - accuracy: 0.3224 - loss: 1.8462\n",
            "Epoch 2/10\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.5253 - loss: 1.3204Epoch 2: Precisión = 54.26%, Error = 45.74%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 87ms/step - accuracy: 0.5253 - loss: 1.3203\n",
            "Epoch 3/10\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.5919 - loss: 1.1526Epoch 3: Precisión = 60.40%, Error = 39.60%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 85ms/step - accuracy: 0.5920 - loss: 1.1525\n",
            "Epoch 4/10\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.6440 - loss: 1.0183Epoch 4: Precisión = 64.61%, Error = 35.39%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 84ms/step - accuracy: 0.6440 - loss: 1.0183\n",
            "Epoch 5/10\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.6707 - loss: 0.9484Epoch 5: Precisión = 67.06%, Error = 32.94%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 86ms/step - accuracy: 0.6707 - loss: 0.9484\n",
            "Epoch 6/10\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.6916 - loss: 0.8839Epoch 6: Precisión = 69.24%, Error = 30.76%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 84ms/step - accuracy: 0.6916 - loss: 0.8839\n",
            "Epoch 7/10\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7100 - loss: 0.8373Epoch 7: Precisión = 70.93%, Error = 29.07%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 84ms/step - accuracy: 0.7100 - loss: 0.8373\n",
            "Epoch 8/10\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.7275 - loss: 0.7781Epoch 8: Precisión = 72.47%, Error = 27.53%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 86ms/step - accuracy: 0.7275 - loss: 0.7781\n",
            "Epoch 9/10\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7391 - loss: 0.7481Epoch 9: Precisión = 73.67%, Error = 26.33%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 82ms/step - accuracy: 0.7391 - loss: 0.7481\n",
            "Epoch 10/10\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7525 - loss: 0.7104Epoch 10: Precisión = 75.09%, Error = 24.91%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 84ms/step - accuracy: 0.7525 - loss: 0.7105\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.7037 - loss: 0.8674\n",
            "\n",
            "Test accuracy sin Data Augmentation: 0.7010999917984009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un generador de imágenes con data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,       # Rotar las imágenes aleatoriamente hasta 20 grados\n",
        "    width_shift_range=0.2,   # Desplazamiento horizontal de hasta el 20% de la imagen\n",
        "    height_shift_range=0.2,  # Desplazamiento vertical de hasta el 20% de la imagen\n",
        "    horizontal_flip=True,    # Voltear horizontalmente las imágenes\n",
        ")\n",
        "\n",
        "# Ajustar el generador de datos al conjunto de entrenamiento\n",
        "datagen.fit(train_images)"
      ],
      "metadata": {
        "id": "BWyTJUy88gTp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo con Data Augmentation\n",
        "print(\"Entrenamiento con Data Augmentation\")\n",
        "history_with_augmentation = model.fit(datagen.flow(train_images, train_labels, batch_size=64),\n",
        "                                      steps_per_epoch=len(train_images) // 64,\n",
        "                                      epochs=10,\n",
        "                                      validation_data=(test_images, test_labels),\n",
        "                                      callbacks=[metrics_callback])\n",
        "\n",
        "# Evaluar el modelo con Data Augmentation\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f'\\nTest accuracy con Data Augmentation: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmLAOHl18hJh",
        "outputId": "b915b062-6b86-4e62-b404-7b918533e68a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenamiento con Data Augmentation\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.5548 - loss: 1.2674Epoch 1: Precisión = 56.57%, Error = 43.43%\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 133ms/step - accuracy: 0.5548 - loss: 1.2673 - val_accuracy: 0.6688 - val_loss: 0.9604\n",
            "Epoch 2/10\n",
            "\u001b[1m  1/781\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 71ms/step - accuracy: 0.5156 - loss: 1.5350"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Precisión = 51.56%, Error = 48.44%\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5156 - loss: 1.5350 - val_accuracy: 0.6754 - val_loss: 0.9455\n",
            "Epoch 3/10\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.5878 - loss: 1.1700Epoch 3: Precisión = 59.11%, Error = 40.89%\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 129ms/step - accuracy: 0.5878 - loss: 1.1700 - val_accuracy: 0.6652 - val_loss: 0.9630\n",
            "Epoch 4/10\n",
            "\u001b[1m  1/781\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 71ms/step - accuracy: 0.5312 - loss: 1.3071Epoch 4: Precisión = 53.12%, Error = 46.88%\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.5312 - loss: 1.3071 - val_accuracy: 0.6721 - val_loss: 0.9391\n",
            "Epoch 5/10\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.5999 - loss: 1.1339Epoch 5: Precisión = 60.39%, Error = 39.61%\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 135ms/step - accuracy: 0.5999 - loss: 1.1339 - val_accuracy: 0.6699 - val_loss: 0.9665\n",
            "Epoch 6/10\n",
            "\u001b[1m  1/781\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 69ms/step - accuracy: 0.6406 - loss: 1.1410Epoch 6: Precisión = 64.06%, Error = 35.94%\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.6406 - loss: 1.1410 - val_accuracy: 0.6809 - val_loss: 0.9308\n",
            "Epoch 7/10\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.6143 - loss: 1.1006Epoch 7: Precisión = 61.39%, Error = 38.61%\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 131ms/step - accuracy: 0.6143 - loss: 1.1006 - val_accuracy: 0.6638 - val_loss: 0.9820\n",
            "Epoch 8/10\n",
            "\u001b[1m  1/781\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 68ms/step - accuracy: 0.6250 - loss: 1.0422Epoch 8: Precisión = 62.50%, Error = 37.50%\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.6250 - loss: 1.0422 - val_accuracy: 0.6625 - val_loss: 0.9847\n",
            "Epoch 9/10\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.6177 - loss: 1.0822Epoch 9: Precisión = 61.76%, Error = 38.24%\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 129ms/step - accuracy: 0.6177 - loss: 1.0822 - val_accuracy: 0.6961 - val_loss: 0.8735\n",
            "Epoch 10/10\n",
            "\u001b[1m  1/781\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 115ms/step - accuracy: 0.6250 - loss: 0.9910Epoch 10: Precisión = 62.50%, Error = 37.50%\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.6250 - loss: 0.9910 - val_accuracy: 0.6960 - val_loss: 0.8673\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.6981 - loss: 0.8631\n",
            "\n",
            "Test accuracy con Data Augmentation: 0.6959999799728394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusión e interpretación\n",
        "Se observa que el modelo alcanzó un 69.60% de precisión en las imágenes de prueba cuando se utilizó Data Augmentation, mientras que sin esta técnica el resultado fue ligeramente mejor, con un 70.10% de precisión. Este comportamiento puede explicarse por la naturaleza del dataset, en el cual las categorías son visualmente distintas entre sí, ya que son aviones, perros, caballos, edificios, camiones, carros... entre otros.\n",
        "\n",
        "El aumento de ejemplos mediante Data Augmentation podría haber introducido variaciones en las imágenes que no ayudaron a mejorar el rendimiento del modelo, ya que este no tenía dificultades significativas para distinguir entre las categorías originales. De hecho, el exceso de imágenes modificadas podría haber añadido complejidad innecesaria al entrenamiento, provocando una ligera disminución en la precisión final.\n",
        "\n",
        "En contextos donde las imágenes son más similares, por ejemplo, distinguir entre perros y lobos, el uso de Data Augmentation podría haber sido más beneficioso, ya que ayudaría al modelo a generalizar mejor. Sin embargo en este ejemplo, como las categorias son tan diferentes esto no ayudo a que generalizada mejor, sino que lo hizo un poco más ineficiente, pero teniendo buenos resultados en ambos casos.\n",
        "\n",
        "AL final de todo esto el Data Augmentation puede ayudar significativamente a crear más ejemplos para que el modelo de redes CNN pueda generalizar mejor, pero en imagenes que si pueda existir confusión, mientras que en este modelo, con categorias tan distintas, no lo ayudo demasiado a pesar de que de nuevo dieroin buenos resultados ambos modelos"
      ],
      "metadata": {
        "id": "e9gefFHD5sgG"
      }
    }
  ]
}