{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiI8YMS5njSS",
        "outputId": "a13bd1a2-f319-4f04-d8bf-7df8fc697afb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.2)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.callbacks import LambdaCallback"
      ],
      "metadata": {
        "id": "h_cNMBC7n5Py"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción\n",
        "En este proyecto, utilizamos un dataset de imágenes obtenido de la librería TensorFlow, ya que las imágenes de Kaggle son demasiado numerosas y pesadas para este análisis. El dataset CIFAR-10 clasifica imágenes en 10 categorías: aviones, autos, pájaros, gatos, venados, perros, ranas, caballos, barcos y camiones. Para mejorar la precisión en la clasificación de estas imágenes, aplicamos Transfer Learning, seleccionando el modelo VGG16, preentrenado en el conjunto de datos ImageNet, que incluye millones de imágenes. Al reutilizar las capas iniciales de este modelo, que ya han aprendido a identificar características visuales generales como bordes y texturas, podemos aprovechar este conocimiento para clasificar con mayor precisión las imágenes de CIFAR-10. También se verá el impacto de usar tanto el Data Augmentation con el Tranfer Learning.\n",
        "# Objetivos del Notebook\n",
        "- Comparar los resultados de los modelos de Data Augmentation con los de tranfer learning, dividiendolos según que se usa\n",
        "- Ver como son los modelos ya preentrenados de Tranfer Learning\n",
        "- Y interpretar los resultados de cada uno de los modelos según su forma de entrenamiento\n"
      ],
      "metadata": {
        "id": "L0pRJxCD2k3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "# Normalizar las imágenes a valores entre 0 y 1 para que se pueda hacer el entrenamiento y pruebas en el modelo, luego de está limpieza\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.astype('float32') / 255"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3YjVU1Xo8Ur",
        "outputId": "26a8c4c5-bef2-4ee1-ff0d-f0a6a46087cf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# En el dataset se puede encontro 50000 imágenes que están distribuidas en las 10 categorías anteriores de 32x32 y a color\n",
        "print(train_images.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uB54l-GzafL",
        "outputId": "8cfa1221-25f7-4897-d4b8-32f701e38698"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un modelo\n",
        "model = models.Sequential()\n",
        "\n",
        "# Primera capa convolucional: 32 filtros de 3x3, activación ReLU, con imágenes de entrada de 32x32x3\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# Segunda capa convolucional con 64 filtros\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# Tercera capa convolucional con 64 filtros\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "# Aplanar la salida para las capas densas\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# Agregar una capa densa con 64 neuronas\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "# Capa de salida con 10 neuronas (correspondiente a las 10 clases de CIFAR-10), usando softmax\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTAe79l_pSa3",
        "outputId": "e4ef733d-5f12-43fb-9f60-f3f5bf73e5d1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Crear un callback para mostrar precisión y error por época\n",
        "metrics_callback = LambdaCallback(on_epoch_end=lambda epoch, logs:\n",
        "                                  print(f'Epoch {epoch + 1}: Precisión = {logs[\"accuracy\"] * 100:.2f}%, Error = {(1 - logs[\"accuracy\"]) * 100:.2f}%'))\n",
        "\n",
        "# Entrenar el modelo sin Data Augmentation\n",
        "print(\"Entrenamiento sin Data Augmentation\")\n",
        "history_no_augmentation = model.fit(train_images, train_labels,\n",
        "                                    epochs=10,\n",
        "                                    batch_size=64,\n",
        "                                    callbacks=[metrics_callback])\n",
        "\n",
        "# Evaluar el modelo sin Data Augmentation\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f'\\nTest accuracy sin Data Augmentation: {test_acc}')\n",
        "\n",
        "# Guardar los pesos iniciales del modelo antes de reiniciarlo\n",
        "initial_weights = model.get_weights()\n",
        "\n",
        "# Reiniciar los pesos del modelo\n",
        "model.set_weights(initial_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SeLNAZKpkot",
        "outputId": "7117bcb4-ee2d-4837-9447-6e2783303009"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenamiento sin Data Augmentation\n",
            "Epoch 1/10\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.3331 - loss: 1.8100Epoch 1: Precisión = 41.71%, Error = 58.29%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 80ms/step - accuracy: 0.3334 - loss: 1.8095\n",
            "Epoch 2/10\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.5442 - loss: 1.2794Epoch 2: Precisión = 56.08%, Error = 43.92%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 79ms/step - accuracy: 0.5443 - loss: 1.2793\n",
            "Epoch 3/10\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.6131 - loss: 1.1000Epoch 3: Precisión = 62.18%, Error = 37.82%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 77ms/step - accuracy: 0.6131 - loss: 1.0999\n",
            "Epoch 4/10\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.6533 - loss: 0.9919Epoch 4: Precisión = 65.97%, Error = 34.03%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 75ms/step - accuracy: 0.6533 - loss: 0.9919\n",
            "Epoch 5/10\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.6816 - loss: 0.9040Epoch 5: Precisión = 68.86%, Error = 31.14%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 79ms/step - accuracy: 0.6817 - loss: 0.9040\n",
            "Epoch 6/10\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7089 - loss: 0.8296Epoch 6: Precisión = 71.13%, Error = 28.87%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 82ms/step - accuracy: 0.7089 - loss: 0.8296\n",
            "Epoch 7/10\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7303 - loss: 0.7719Epoch 7: Precisión = 72.94%, Error = 27.06%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 75ms/step - accuracy: 0.7303 - loss: 0.7719\n",
            "Epoch 8/10\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7467 - loss: 0.7257Epoch 8: Precisión = 74.58%, Error = 25.42%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 79ms/step - accuracy: 0.7467 - loss: 0.7257\n",
            "Epoch 9/10\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7611 - loss: 0.6798Epoch 9: Precisión = 75.91%, Error = 24.09%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 76ms/step - accuracy: 0.7611 - loss: 0.6798\n",
            "Epoch 10/10\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.7778 - loss: 0.6322Epoch 10: Precisión = 77.12%, Error = 22.88%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 75ms/step - accuracy: 0.7778 - loss: 0.6322\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.7148 - loss: 0.8491\n",
            "\n",
            "Test accuracy sin Data Augmentation: 0.7092999815940857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un generador de imágenes con data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,       # Rotar las imágenes aleatoriamente hasta 20 grados\n",
        "    width_shift_range=0.2,   # Desplazamiento horizontal de hasta el 20% de la imagen\n",
        "    height_shift_range=0.2,  # Desplazamiento vertical de hasta el 20% de la imagen\n",
        "    horizontal_flip=True,    # Voltear horizontalmente las imágenes\n",
        ")\n",
        "\n",
        "# Ajustar el generador de datos al conjunto de entrenamiento\n",
        "datagen.fit(train_images)"
      ],
      "metadata": {
        "id": "BWyTJUy88gTp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo con Data Augmentation\n",
        "print(\"Entrenamiento con Data Augmentation\")\n",
        "history_with_augmentation = model.fit(datagen.flow(train_images, train_labels, batch_size=64),\n",
        "                                      steps_per_epoch=len(train_images) // 64,\n",
        "                                      epochs=10,\n",
        "                                      validation_data=(test_images, test_labels),\n",
        "                                      callbacks=[metrics_callback])\n",
        "\n",
        "# Evaluar el modelo con Data Augmentation\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f'\\nTest accuracy con Data Augmentation: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmLAOHl18hJh",
        "outputId": "3e04429c-5168-4cd9-b825-f8cbcd5a3827"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenamiento con Data Augmentation\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.5623 - loss: 1.2432Epoch 1: Precisión = 57.75%, Error = 42.25%\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 118ms/step - accuracy: 0.5624 - loss: 1.2432 - val_accuracy: 0.6817 - val_loss: 0.9159\n",
            "Epoch 2/10\n",
            "\u001b[1m  1/781\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 62ms/step - accuracy: 0.5781 - loss: 0.9421"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Precisión = 57.81%, Error = 42.19%\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5781 - loss: 0.9421 - val_accuracy: 0.6738 - val_loss: 0.9433\n",
            "Epoch 3/10\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.5996 - loss: 1.1350Epoch 3: Precisión = 60.33%, Error = 39.67%\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 121ms/step - accuracy: 0.5996 - loss: 1.1350 - val_accuracy: 0.6534 - val_loss: 1.0102\n",
            "Epoch 4/10\n",
            "\u001b[1m  1/781\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 64ms/step - accuracy: 0.5938 - loss: 1.1047Epoch 4: Precisión = 59.38%, Error = 40.62%\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5938 - loss: 1.1047 - val_accuracy: 0.6595 - val_loss: 0.9809\n",
            "Epoch 5/10\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.6120 - loss: 1.0980Epoch 5: Precisión = 61.59%, Error = 38.41%\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 121ms/step - accuracy: 0.6120 - loss: 1.0980 - val_accuracy: 0.6395 - val_loss: 1.0814\n",
            "Epoch 6/10\n",
            "\u001b[1m  1/781\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 61ms/step - accuracy: 0.7031 - loss: 0.9108Epoch 6: Precisión = 70.31%, Error = 29.69%\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7031 - loss: 0.9108 - val_accuracy: 0.6437 - val_loss: 1.0759\n",
            "Epoch 7/10\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.6204 - loss: 1.0698Epoch 7: Precisión = 62.55%, Error = 37.45%\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 121ms/step - accuracy: 0.6204 - loss: 1.0698 - val_accuracy: 0.6692 - val_loss: 0.9578\n",
            "Epoch 8/10\n",
            "\u001b[1m  1/781\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 62ms/step - accuracy: 0.6719 - loss: 1.0278Epoch 8: Precisión = 67.19%, Error = 32.81%\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6719 - loss: 1.0278 - val_accuracy: 0.6708 - val_loss: 0.9666\n",
            "Epoch 9/10\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.6354 - loss: 1.0314Epoch 9: Precisión = 63.55%, Error = 36.45%\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 119ms/step - accuracy: 0.6354 - loss: 1.0314 - val_accuracy: 0.6935 - val_loss: 0.8847\n",
            "Epoch 10/10\n",
            "\u001b[1m  1/781\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 61ms/step - accuracy: 0.6094 - loss: 1.0489Epoch 10: Precisión = 60.94%, Error = 39.06%\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6094 - loss: 1.0489 - val_accuracy: 0.6906 - val_loss: 0.8935\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.6920 - loss: 0.8879\n",
            "\n",
            "Test accuracy con Data Augmentation: 0.6905999779701233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo preentrenado VGG16 sin las capas superiores\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "base_model.trainable = False  # Congelar las capas\n",
        "\n",
        "# Crear un modelo con las capas adicionales\n",
        "model_transfer = models.Sequential([\n",
        "    base_model,\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model_transfer.compile(optimizer='adam',\n",
        "                       loss='sparse_categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "print(\"\\nEntrenamiento del modelo CON Transfer Learning (SIN Data Augmentation):\")\n",
        "model_transfer.fit(train_images, train_labels, epochs=10, batch_size=64, callbacks=[metrics_callback])\n",
        "\n",
        "# Evaluar el modelo\n",
        "test_loss_transfer, test_acc_transfer = model_transfer.evaluate(test_images, test_labels)\n",
        "print(f'\\nTest accuracy (CON Transfer Learning SIN Data Augmentation): {test_acc_transfer}')\n",
        "\n",
        "# Entrenar el modelo transfer learning con data augmentation\n",
        "print(\"\\nEntrenamiento del modelo CON Transfer Learning y CON Data Augmentation:\")\n",
        "model_transfer.fit(datagen.flow(train_images, train_labels, batch_size=64),\n",
        "                   epochs=10,\n",
        "                   validation_data=(test_images, test_labels),\n",
        "                   callbacks=[metrics_callback])\n",
        "\n",
        "# Evaluar el modelo\n",
        "test_loss_transfer_aug, test_acc_transfer_aug = model_transfer.evaluate(test_images, test_labels)\n",
        "print(f'\\nTest accuracy (CON Transfer Learning y CON Data Augmentation): {test_acc_transfer_aug}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBvCg1f4f5Pa",
        "outputId": "a7df38e7-4a5f-4415-e1bb-9be1daa8f975"
      },
      "execution_count": 9,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "Entrenamiento del modelo CON Transfer Learning (SIN Data Augmentation):\n",
            "Epoch 1/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700ms/step - accuracy: 0.3973 - loss: 1.7092Epoch 1: Precisión = 46.86%, Error = 53.14%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m550s\u001b[0m 700ms/step - accuracy: 0.3974 - loss: 1.7089\n",
            "Epoch 2/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691ms/step - accuracy: 0.5431 - loss: 1.3104Epoch 2: Precisión = 54.56%, Error = 45.44%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 691ms/step - accuracy: 0.5431 - loss: 1.3103\n",
            "Epoch 3/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691ms/step - accuracy: 0.5635 - loss: 1.2484Epoch 3: Precisión = 56.53%, Error = 43.47%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m541s\u001b[0m 691ms/step - accuracy: 0.5635 - loss: 1.2483\n",
            "Epoch 4/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692ms/step - accuracy: 0.5765 - loss: 1.2120Epoch 4: Precisión = 57.66%, Error = 42.34%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m541s\u001b[0m 692ms/step - accuracy: 0.5765 - loss: 1.2120\n",
            "Epoch 5/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689ms/step - accuracy: 0.5861 - loss: 1.1847Epoch 5: Precisión = 58.51%, Error = 41.49%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m538s\u001b[0m 689ms/step - accuracy: 0.5861 - loss: 1.1847\n",
            "Epoch 6/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694ms/step - accuracy: 0.5927 - loss: 1.1683Epoch 6: Precisión = 59.29%, Error = 40.71%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m542s\u001b[0m 694ms/step - accuracy: 0.5927 - loss: 1.1683\n",
            "Epoch 7/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688ms/step - accuracy: 0.6045 - loss: 1.1373Epoch 7: Precisión = 60.08%, Error = 39.92%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 688ms/step - accuracy: 0.6045 - loss: 1.1373\n",
            "Epoch 8/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688ms/step - accuracy: 0.6037 - loss: 1.1319Epoch 8: Precisión = 60.46%, Error = 39.54%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m538s\u001b[0m 688ms/step - accuracy: 0.6037 - loss: 1.1319\n",
            "Epoch 9/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691ms/step - accuracy: 0.6123 - loss: 1.1101Epoch 9: Precisión = 60.91%, Error = 39.09%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m564s\u001b[0m 691ms/step - accuracy: 0.6123 - loss: 1.1101\n",
            "Epoch 10/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742ms/step - accuracy: 0.6144 - loss: 1.1003Epoch 10: Precisión = 61.25%, Error = 38.75%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m602s\u001b[0m 742ms/step - accuracy: 0.6144 - loss: 1.1003\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 358ms/step - accuracy: 0.6106 - loss: 1.1048\n",
            "\n",
            "Test accuracy (CON Transfer Learning SIN Data Augmentation): 0.6128000020980835\n",
            "\n",
            "Entrenamiento del modelo CON Transfer Learning y CON Data Augmentation:\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725ms/step - accuracy: 0.5014 - loss: 1.4289Epoch 1: Precisión = 50.60%, Error = 49.40%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m676s\u001b[0m 865ms/step - accuracy: 0.5014 - loss: 1.4289 - val_accuracy: 0.5867 - val_loss: 1.1852\n",
            "Epoch 2/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721ms/step - accuracy: 0.5164 - loss: 1.3776Epoch 2: Precisión = 51.58%, Error = 48.42%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m672s\u001b[0m 859ms/step - accuracy: 0.5164 - loss: 1.3776 - val_accuracy: 0.5816 - val_loss: 1.1841\n",
            "Epoch 3/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721ms/step - accuracy: 0.5139 - loss: 1.3787Epoch 3: Precisión = 51.89%, Error = 48.11%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m716s\u001b[0m 903ms/step - accuracy: 0.5139 - loss: 1.3787 - val_accuracy: 0.5815 - val_loss: 1.1823\n",
            "Epoch 4/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722ms/step - accuracy: 0.5195 - loss: 1.3664Epoch 4: Precisión = 51.92%, Error = 48.08%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m708s\u001b[0m 858ms/step - accuracy: 0.5195 - loss: 1.3664 - val_accuracy: 0.5866 - val_loss: 1.1673\n",
            "Epoch 5/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725ms/step - accuracy: 0.5279 - loss: 1.3382Epoch 5: Precisión = 52.35%, Error = 47.65%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m683s\u001b[0m 861ms/step - accuracy: 0.5279 - loss: 1.3382 - val_accuracy: 0.5877 - val_loss: 1.1699\n",
            "Epoch 6/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722ms/step - accuracy: 0.5279 - loss: 1.3457Epoch 6: Precisión = 52.82%, Error = 47.18%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m715s\u001b[0m 904ms/step - accuracy: 0.5279 - loss: 1.3457 - val_accuracy: 0.5833 - val_loss: 1.1837\n",
            "Epoch 7/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724ms/step - accuracy: 0.5271 - loss: 1.3372Epoch 7: Precisión = 52.84%, Error = 47.16%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m709s\u001b[0m 906ms/step - accuracy: 0.5271 - loss: 1.3372 - val_accuracy: 0.5817 - val_loss: 1.1902\n",
            "Epoch 8/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723ms/step - accuracy: 0.5257 - loss: 1.3359Epoch 8: Precisión = 52.68%, Error = 47.32%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m705s\u001b[0m 859ms/step - accuracy: 0.5257 - loss: 1.3359 - val_accuracy: 0.5905 - val_loss: 1.1596\n",
            "Epoch 9/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722ms/step - accuracy: 0.5323 - loss: 1.3259Epoch 9: Precisión = 52.96%, Error = 47.04%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m673s\u001b[0m 860ms/step - accuracy: 0.5323 - loss: 1.3259 - val_accuracy: 0.5919 - val_loss: 1.1687\n",
            "Epoch 10/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721ms/step - accuracy: 0.5322 - loss: 1.3237Epoch 10: Precisión = 53.35%, Error = 46.65%\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m671s\u001b[0m 857ms/step - accuracy: 0.5322 - loss: 1.3237 - val_accuracy: 0.5837 - val_loss: 1.1768\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 363ms/step - accuracy: 0.5814 - loss: 1.1811\n",
            "\n",
            "Test accuracy (CON Transfer Learning y CON Data Augmentation): 0.5837000012397766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusión e interpretación\n",
        "Los modelos sacaron los siguientes resultados finales en los datos de prueba: Modelo sin Data Augmentation y sin Transfer Learning dio igual a 70.90% de precisión, Modelo con Data Augmentation y sin Transfer Learning dio una precisión de 69.05%, Modelo sin Data Augmentation y con Transfer Learning fue de 61.28% y el último Modelo con Data Augmentation y con Transfer Learning dio un resultado de precisión de 58.37%.\n",
        "\n",
        "En la explicación, en el caso del Data Augmentation, ya se comentó que esta técnica es muy útil en el caso de imágenes que tengan similitudes muy notables, como tal vez las razas de perros, pero en el caso de este dataset, las categorías son 10 y muy distintas entre cada una, por lo que generar más ejemplos de los que ya tenemos puede resultar contraproducente a la hora de entrenar el modelo. Aun así, sin meter el Transfer Learning, estos sin ninguna de las dos técnicas y solo con el Data Augmentation dieron muy buenos resultados, con una variación muy baja a la hora de la prueba, por lo que los modelos en este caso se mantuvieron bien.\n",
        "\n",
        "Ya cuando incluimos el Transfer Learning se puede ver una disminución muy grande en la precisión final del modelo en los resultados finales, y esto puede ser por el hecho de que el modelo que escogimos no estaba entrenado con todas las categorías del CIFAR-10, por lo que al ser tantas categorías puede que sea muy preciso para ciertas imágenes, pero muy malo para otras. Esto puede explicar su bajo rendimiento. Lo mismo sucede cuando agregamos el Data Augmentation a la ecuación, ya que desde un inicio el modelo está mal; esta técnica lo que hace es confundirlo más, por lo que da los malos resultados que se nos presentaron anteriormente en ambos casos.\n",
        "\n",
        "Por lo que en el caso del Transfer Learning es bueno utilizarlo, pero en este caso capaz se escogió un modelo ya preentrenado que no contaba con la información suficiente de las categorías que se querían utilizar, por lo que se confundió más y ya no hablemos de agregarle el Data Augmentation. Por lo que sí se vio cómo funciona el Transfer Learning, solo que no se supo apreciar bien en sus resultados de precisión."
      ],
      "metadata": {
        "id": "e9gefFHD5sgG"
      }
    }
  ]
}